{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 8 Assignment Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/06/07 00:15:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "\n",
      "\n",
      "Deleted hdfs://localhost:9000/tmp/flightData_in\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "drwxrwxrwx   - root root          0 2020-06-06 01:25 hdfs://localhost:9000/tmp/flightData_in/flightDelay.parquet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -chmod -R 777 hdfs://localhost:9000/tmp\n",
    "! hadoop fs -rm    -r  hdfs://localhost:9000/tmp/flightData_*\n",
    "! hadoop fs -mkdir -p  hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -put   -p  flightDelay.parquet hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -ls        hdfs://localhost:9000/tmp/flightData_in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey fs.defaultFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://1c0c4af6b969:4042\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591488959724)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder, PCA}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, Multic..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Start a simple Spark Session\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "//Feature Processing Classes\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder, PCA}\n",
    "\n",
    "//Linear Algebra Data Structures\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "\n",
    "//Model Building Pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "//Binary Classification\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel,\n",
    "                                           RandomForestClassifier, GBTClassifier,\n",
    "                                           DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
    "//Model Training\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, \n",
    "                                   ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "//Model Evaluation\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "\n",
    "//Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "//For Cleaning\n",
    "//import scala.util.matching.Regex\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Group 8 ML Phase 3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a parquet file of flight delay, fuel-price and meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Departing_Port: string (nullable = true)\n",
      " |-- Arriving_Port: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month_Num: string (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- Departing_Port_station_ID: string (nullable = true)\n",
      " |-- Departing_Port_station_name: string (nullable = true)\n",
      " |-- Arriving_Port_station_ID: string (nullable = true)\n",
      " |-- Arriving_Port_station_name: string (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Depart: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Depart: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Depart: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Depart: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Depart: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Depart: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Arrive: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Arrive: double (nullable = true)\n",
      " |-- Date_Num: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flights: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights = (spark\n",
    "            .read.parquet(\"flightDelay.parquet\")\n",
    "            .withColumn(\"Month_Num1\", $\"Month_Num\" cast \"Int\")\n",
    "            //convert month and year to integer index starting Jan 2004\n",
    "            .withColumn(\"Date_Num\",  ($\"Year\"-2004)*12 + $\"Month_Num1\")\n",
    "            .drop(\"Sectors_Flown\", \"Month_Num1\", \"Change\")\n",
    "            .withColumnRenamed(\"Departures_Delayed\",\"label\")\n",
    "            .withColumnRenamed(\"Price\",\"Fuel_Price\")\n",
    "            //drop NA's even though none were found!\n",
    "            .na.drop()\n",
    "            //.cache\n",
    "              )\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the degree of imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of lates (label=1) in the sample\n",
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|1072071|\n",
      "|    0|5224826|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counts: org.apache.spark.sql.DataFrame = [label: int, count: bigint]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counts = flights.groupBy(\"label\").count()\n",
    "\n",
    "println(\"proportion of lates (label=1) in the sample\")\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split The Data into training and testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set of the Most Recent 12 Months has 435479 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawTesting: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "rawTraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out the most recent 12 months of flight data as the test dataset\n",
    "//Dates after March 2019 have Date_Num > 183\n",
    "val rawTesting = flights.filter($\"Date_Num\"> 183)\n",
    "println(s\"Test Set of the Most Recent 12 Months has ${rawTesting.count()} records\")\n",
    "\n",
    "//Filter out rows prior to the most recent 12 months of flight data as the training dataset\n",
    "val rawTraining = flights.filter($\"Date_Num\" < 184)\n",
    "\n",
    "//val Array(imbalancedTraining, testing) = flights.randomSplit(Array(0.7, 0.3), seed = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down sample the Ontime Departures To Balance The Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On time Training Flights: 4884963\n",
      "Delayed Training Flights: 976455\n",
      "Down Sampled ontime Training Flights: 976479\n",
      "proportion of lates (label=1) in the sample\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|97279|\n",
      "|    0|97701|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ontimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "delayedTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampledOntimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampleFraction: Double = 0.1\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "resampledCounts: org.apache.spark.sql.DataFrame = [l..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ontimeTrainingFlights = rawTraining.filter($\"label\"===0)\n",
    "println(s\"On time Training Flights: ${ontimeTrainingFlights.count()}\")\n",
    "\n",
    "val delayedTrainingFlights = rawTraining.filter($\"label\"===1)\n",
    "println(s\"Delayed Training Flights: ${delayedTrainingFlights.count()}\")\n",
    "\n",
    "val sampledOntimeTrainingFlights = ontimeTrainingFlights.sample(false, 0.2)  \n",
    "\n",
    "println(s\"Down Sampled ontime Training Flights: ${sampledOntimeTrainingFlights.count()}\")\n",
    "\n",
    "\n",
    "val sampleFraction = 0.1\n",
    "//Concatenate rows of ontimeTrainingFlights and delayedTrainingFlights\n",
    "val training = (sampledOntimeTrainingFlights\n",
    "                .union(delayedTrainingFlights)\n",
    "                .sample(false, sampleFraction))\n",
    "\n",
    "val testing = rawTesting.sample(false, 0.1)\n",
    "               \n",
    "val resampledCounts = training.groupBy(\"label\").count()\n",
    "println(\"proportion of lates (label=1) in the sample\")\n",
    "resampledCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment via a Confusion Matrix Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getConfusionMatrix: (predictionDF: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getConfusionMatrix(predictionDF: DataFrame): Unit = {\n",
    "    val eval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "    println(s\"Accuracy: ${eval.setMetricName(\"accuracy\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Precision: ${eval.setMetricName(\"weightedPrecision\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Recall: ${eval.setMetricName(\"weightedRecall\").evaluate(predictionDF)}\")\n",
    "    println(s\"F1: ${eval.setMetricName(\"f1\").evaluate(predictionDF)}\")\n",
    "\n",
    "    val TP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "    val TN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "    val FP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "    val FN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "    val total = predictionDF.select(\"label\").count.toDouble\n",
    "    // Unweighted Metrics\n",
    "    val accuracy    = (TP + TN) / total\n",
    "    val precision   = TP / (TP+FP).toDouble\n",
    "    val recall      = TP / (TP+FN).toDouble\n",
    "    val F1 = 2/(1/precision + 1/recall)\n",
    "    println(s\"Accuracy: ${accuracy}\")\n",
    "    println(s\"Precision: ${precision}\")\n",
    "    println(s\"Recall: ${recall}\")\n",
    "    println(s\"F1: ${F1}\")\n",
    "\n",
    "    // Confusion matrix\n",
    "    printf(s\"\"\"|=================== Confusion Matrix ==========================\n",
    "           |##########| %-15s                     %-15s\n",
    "           |----------+----------------------------------------------------\n",
    "           |Actual = 0| %-15d                     %-15d\n",
    "           |Actual = 1| %-15d                     %-15d\n",
    "           |===============================================================\n",
    "         \"\"\".stripMargin, \"Predicted = 0\", \"Predicted = 1\", TN, FP, FN, TP)\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the ML Pipleline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalVariables: Array[String] = Array(Departing_Port, Arriving_Port, Airline)\n",
       "categoricalIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_3a39d6d84025, strIdx_78428a9cdbb9, strIdx_f6be01bdc8e7)\n",
       "categoricalEncoders: Array[org.apache.spark.ml.feature.OneHotEncoder] = Array(oneHot_94946b4dedcc, oneHot_57f0d0c82872, oneHot_53f5f0f32577)\n",
       "cols: Array[String] = Array(Date_Num, Airline_Vec, Fuel_Price, Departing_Port_Vec, Mean_daily_wind_run_km_Depart, Mean_rainfall_mm_Depart, Mean_number_of_days_of_rain_Depart, Mean_number_of_days_>_40_Degrees_C_Depart, Arriving_Port_Vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7b3e1d11a772\n",
       "pca: org.apache.spark.ml.feature.PCA = pca_58286315bfac\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//////////////////////////////////////////////////\n",
    "//// Setting Up DataFrame for Machine Learning ///\n",
    "//////////////////////////////////////////////////\n",
    "\n",
    "// Deal with Categorical Columns\n",
    "val categoricalVariables = Array(\n",
    "    \"Departing_Port\", \"Arriving_Port\", \"Airline\")\n",
    "val categoricalIndexers = categoricalVariables\n",
    "  .map(i => new StringIndexer().setInputCol(i).setOutputCol(i+\"_Index\"))\n",
    "val categoricalEncoders = categoricalVariables\n",
    "  .map(e => new OneHotEncoder().setInputCol(e + \"_Index\").setOutputCol(e + \"_Vec\"))\n",
    "\n",
    "\n",
    "// columns that need to be added to the features vector\n",
    "val cols = Array(\"Date_Num\",  \"Airline_Vec\", \"Fuel_Price\",\n",
    "    \"Departing_Port_Vec\", \"Mean_daily_wind_run_km_Depart\", \"Mean_rainfall_mm_Depart\",\n",
    "    \"Mean_number_of_days_of_rain_Depart\",\"Mean_number_of_days_>_40_Degrees_C_Depart\",\n",
    "    \"Arriving_Port_Vec\")\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "                 .setInputCols(cols)\n",
    "                 .setOutputCol(\"indexedFeatures\"))\n",
    "\n",
    "// Choose linear combinations of explanatory variables that explain the most variance in the training data\n",
    "val pca = new PCA()\n",
    "    .setInputCol(assembler.getOutputCol)\n",
    "        //\"indexedFeatures\")\n",
    "    .setOutputCol(\"features\").setK(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest parameters:\n",
      " cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto, current: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini, current: gini)\n",
      "labelCol: label column name (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 3)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1. (default: 1)\n",
      "numTrees: Number of trees to train (at least 1) (default: 20, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "seed: random seed (default: 207336481, current: 12345)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "randomForestClassifier: org.apache.spark.ml.classification.RandomForestClassifier = rfc_a489eacb088b\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create a random forest classifier\n",
    "\n",
    "val randomForestClassifier = new RandomForestClassifier()\n",
    "  .setImpurity(\"gini\")\n",
    "  .setMaxDepth(3)\n",
    "  .setNumTrees(10)\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "  .setSeed(12345)\n",
    "\n",
    "// Print out the parameters, documentation, and any default values.\n",
    "println(s\"Random Forest parameters:\\n ${randomForestClassifier.explainParams()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stages: Array[org.apache.spark.ml.PipelineStage] = Array(strIdx_3a39d6d84025, strIdx_78428a9cdbb9, strIdx_f6be01bdc8e7, oneHot_94946b4dedcc, oneHot_57f0d0c82872, oneHot_53f5f0f32577, vecAssembler_7b3e1d11a772, pca_58286315bfac, rfc_a489eacb088b)\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_d20db20c2824\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Initialise stages for the pipline\n",
    "val stages: Array[PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler, pca, randomForestClassifier)\n",
    "\n",
    "// build the pipeline\n",
    "val pipeline = new Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val rfpipemodel = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// // test the model with test data\n",
    "// val rfpipeprediction = rfpipemodel.transform(testing)\n",
    "\n",
    "// rfpipeprediction.select (\"features\", \"probability\", \"prediction\", \"label\").show(10)\n",
    "\n",
    "// // Measure the quality of the predictions\n",
    "// getConfusionMatrix(rfpipeprediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Logistic Regression Pipeline using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_a489eacb088b-impurity: entropy,\n",
       "\trfc_a489eacb088b-maxBins: 3,\n",
       "\trfc_a489eacb088b-maxDepth: 4\n",
       "}, {\n",
       "\trfc_a489eacb088b-impurity: entropy,\n",
       "\trfc_a489eacb088b-maxBins: 5,\n",
       "\trfc_a489eacb088b-maxDepth: 4\n",
       "}, {\n",
       "\trfc_a489eacb088b-impurity: gini,\n",
       "\trfc_a489eacb088b-maxBins: 3,\n",
       "\trfc_a489eacb088b-maxDepth: 4\n",
       "}, {\n",
       "\trfc_a489eacb088b-impurity: gini,\n",
       "\trfc_a489eacb088b-maxBins: 5,\n",
       "\trfc_a489eacb088b-maxDepth: 4\n",
       "}, {\n",
       "\trfc_a489eacb088b-impurity: entropy,\n",
       "\trfc_a489eacb088b-maxBins: 3,\n",
       "\trfc_a489eacb088b-maxDepth: 5\n",
       "}, {\n",
       "\trfc_a489eacb088b-impurity: entropy,\n",
       "\trfc_a489eacb088b-maxBins: 5,\n",
       "\trfc_a489eacb088b-maxDepth: 5\n",
       "}, {\n",
       "\trfc_a489eacb088b-impurity: gini,\n",
       "\trfc_a489eacb088b-maxBins: 3,\n",
       "\trfc_a489eacb088b-maxDepth: 5\n",
       "}, {\n",
       "\trfc_a489eacb088b-impurity: g..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(randomForestClassifier.maxBins, Array(3,5))\n",
    "  .addGrid(randomForestClassifier.maxDepth, Array(4,5))\n",
    "  .addGrid(randomForestClassifier.impurity, Array(\"entropy\", \"gini\"))\n",
    "  .build()\n",
    "\n",
    "//Unable to go to the extend required on current machine - ideally this would be the CV\n",
    "// val paramGrid = new ParamGridBuilder()\n",
    "//   .addGrid(randomForestClassifier.maxBins, Array(20,30,50))\n",
    "//   .addGrid(randomForestClassifier.maxDepth, Array(3,6,9))\n",
    "//   .addGrid(randomForestClassifier.impurity, Array(\"entropy\", \"gini\"))\n",
    "//   .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_d77c502e3b99\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "// This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "// is areaUnderROC.\n",
    "\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(5)  // Use 3+ in practice\n",
    "  .setParallelism(2)  // Evaluate up to 2 parameter settings in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 00:18:54,768 WARN  [CrossValidator-thread-pool-1] netlib.LAPACK (LAPACK.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2020-06-07 00:18:54,769 WARN  [CrossValidator-thread-pool-1] netlib.LAPACK (LAPACK.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = cv_d77c502e3b99\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParamMap: {\n",
      "\tcv_d77c502e3b99-collectSubModels: false,\n",
      "\tcv_d77c502e3b99-estimator: pipeline_d20db20c2824,\n",
      "\tcv_d77c502e3b99-estimatorParamMaps: [Lorg.apache.spark.ml.param.ParamMap;@37c3165c,\n",
      "\tcv_d77c502e3b99-evaluator: binEval_1b45d4d593ea,\n",
      "\tcv_d77c502e3b99-numFolds: 5,\n",
      "\tcv_d77c502e3b99-parallelism: 2,\n",
      "\tcv_d77c502e3b99-seed: -1191137437\n",
      "}\n",
      "Max Bins: "
     ]
    },
    {
     "ename": "<console>",
     "evalue": "61: warning: fruitless type test: a value of type org.apache.spark.ml.Transformer cannot also be a org.apache.spark.ml.classification.RandomForestClassifier",
     "output_type": "error",
     "traceback": [
      "<console>:61: warning: fruitless type test: a value of type org.apache.spark.ml.Transformer cannot also be a org.apache.spark.ml.classification.RandomForestClassifier",
      "         .map(_.stages.collect { case ml: RandomForestClassifier => ml })",
      "                                          ^",
      "java.util.NoSuchElementException: None.get",
      "  at scala.None$.get(Option.scala:347)",
      "  at scala.None$.get(Option.scala:345)",
      "  ... 45 elided",
      ""
     ]
    }
   ],
   "source": [
    "// Print the coefficients and intercept for logistic regression\n",
    "//println(s\"*******************************************\\nCoefficients: ${model.coefficients} Intercept: ${model.intercept}\")\n",
    "// Since model is a Model (i.e., a Transformer produced by an Estimator),\n",
    "// we can view the parameters it used during fit().\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this LogisticRegression instance.\n",
    "\n",
    "println(s\"ParamMap: ${model.parent.extractParamMap}\")\n",
    "\n",
    "//Get Coeffs of the Best Logistic Regression Model\n",
    "val bestModel = model.bestModel match {\n",
    "  case pm: PipelineModel => Some(pm)\n",
    "  case _ => None\n",
    "}\n",
    "\n",
    "val ml = bestModel\n",
    "  .map(_.stages.collect { case ml: RandomForestClassifier => ml })\n",
    "  .flatMap(_.headOption)\n",
    "\n",
    "// Get best CV Model Parameters\n",
    "print(\"Max Bins: \")\n",
    "println(ml.map(m => (m.maxBins)).get)\n",
    "print(\"Max Depth: \")\n",
    "println(ml.map(m => (m.maxDepth)).get)\n",
    "print(\"Impurity: \")\n",
    "println(ml.map(m => m.impurity).get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-----+\n",
      "|            features|         probability|prediction|label|\n",
      "+--------------------+--------------------+----------+-----+\n",
      "|[228.390185669663...|[0.45034270041792...|       1.0|    1|\n",
      "|[228.390790219345...|[0.45034270041792...|       1.0|    1|\n",
      "|[228.390790219345...|[0.45034270041792...|       1.0|    1|\n",
      "|[228.390790219345...|[0.45034270041792...|       1.0|    1|\n",
      "|[228.390790219345...|[0.45034270041792...|       1.0|    1|\n",
      "|[213.818565039766...|[0.45731710200474...|       1.0|    1|\n",
      "|[213.819169589448...|[0.45731710200474...|       1.0|    1|\n",
      "|[213.819169589448...|[0.45731710200474...|       1.0|    1|\n",
      "|[214.837604819055...|[0.46170091161084...|       1.0|    1|\n",
      "|[214.837604819055...|[0.46170091161084...|       1.0|    1|\n",
      "+--------------------+--------------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy: 0.42503046607344047\n",
      "Weighted Precision: 0.6878150962030158\n",
      "Weighted Recall: 0.4250304660734405\n",
      "F1: 0.4567108228847872\n",
      "Accuracy: 0.42503046607344047\n",
      "Precision: 0.2289698605488079\n",
      "Recall: 0.7083824001712878\n",
      "F1: 0.34607740585774055\n",
      "=================== Confusion Matrix ==========================\n",
      "##########| Predicted = 0                       Predicted = 1  \n",
      "----------+----------------------------------------------------\n",
      "Actual = 0| 11868                               22282          \n",
      "Actual = 1| 2724                                6617           \n",
      "===============================================================\n",
      "         "
     ]
    },
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 83 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get Results on Test Set\n",
    "val results = model.transform(testing)\n",
    "\n",
    "// Take a look at the predictions\n",
    "results.select (\"features\", \"probability\", \"prediction\", \"label\").show(10)\n",
    "\n",
    "// Measure the quality of the predictions\n",
    "getConfusionMatrix(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store The Best CV Trained Logistic Model for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Persist the Model\n",
    "\n",
    "model\n",
    "    .write\n",
    "    .overwrite()\n",
    "    .save(\"./flightDelayModel/\")\n",
    "\n",
    "val results: DataFrame = CrossValidatorModel\n",
    ".load(\"./flightDelayModel/\")\n",
    ".transform(testing)\n",
    ".select(\n",
    "    col(\"features\"),\n",
    "    col(\"label\"),\n",
    "    col(\"prediction\")\n",
    ")\n",
    "\n",
    "//results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
